{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "806074e2",
      "metadata": {
        "id": "806074e2"
      },
      "source": [
        "# Project: Question-Answering on Private Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e2b81b0-3659-4d30-9731-aa084c3a415a",
      "metadata": {
        "id": "0e2b81b0-3659-4d30-9731-aa084c3a415a"
      },
      "outputs": [],
      "source": [
        "#pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6899bf0-200a-4bac-a5ba-0ccf9b2b1ace",
      "metadata": {
        "id": "f6899bf0-200a-4bac-a5ba-0ccf9b2b1ace"
      },
      "outputs": [],
      "source": [
        "#pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc52fd54",
      "metadata": {
        "id": "bc52fd54",
        "outputId": "7960427e-b403-4f6b-814a-7d5242c65357"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'openai' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m api_key \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      7\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(openai_api_key\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[1;32m----> 9\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key  \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'openai' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import langchain_openai\n",
        "from langchain_openai import OpenAI\n",
        "import sys\n",
        "f = open(r'D:\\Generative_AI\\OpenAI\\openai_key.txt')\n",
        "api_key = f.read()\n",
        "llm = OpenAI(openai_api_key=api_key)\n",
        "\n",
        "openai.api_key  = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9a08f54",
      "metadata": {
        "id": "c9a08f54"
      },
      "source": [
        "### Loading Different Document Formats\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db3ffe35",
      "metadata": {
        "id": "db3ffe35"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e86fba",
      "metadata": {
        "id": "a0e86fba"
      },
      "outputs": [],
      "source": [
        "# loading PDF, DOCX and TXT files as LangChain Documents\n",
        "def load_document(file):\n",
        "    import os\n",
        "    name, extension = os.path.splitext(file)\n",
        "\n",
        "    if extension == '.pdf':\n",
        "        from langchain.document_loaders import PyPDFLoader\n",
        "        print(f'Loading {file}')\n",
        "        loader = PyPDFLoader(file)\n",
        "    elif extension == '.docx':\n",
        "        from langchain.document_loaders import Docx2txtLoader\n",
        "        print(f'Loading {file}')\n",
        "        loader = Docx2txtLoader(file)\n",
        "    elif extension == '.txt':\n",
        "        from langchain.document_loaders import TextLoader\n",
        "        loader = TextLoader(file)\n",
        "    else:\n",
        "        print('Document format is not supported!')\n",
        "        return None\n",
        "\n",
        "    data = loader.load()\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f09eac8",
      "metadata": {
        "id": "8f09eac8"
      },
      "outputs": [],
      "source": [
        "def chunk_data(data, chunk_size=256):\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
        "    chunks = text_splitter.split_documents(data)\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf64e722",
      "metadata": {
        "scrolled": true,
        "id": "bf64e722"
      },
      "outputs": [],
      "source": [
        "def print_embedding_cost(texts):\n",
        "    import tiktoken\n",
        "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
        "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
        "    print(f'Total Tokens: {total_tokens}')\n",
        "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0004:.6f}')\n",
        "\n",
        "\n",
        "# print_embedding_cost(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bc87f67",
      "metadata": {
        "id": "9bc87f67"
      },
      "outputs": [],
      "source": [
        "# create embeddings using OpenAIEmbeddings() and save them in a Chroma vector store\n",
        "def create_embeddings(chunks):\n",
        "    from langchain.vectorstores import Chroma\n",
        "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vector_store = Chroma.from_documents(chunks, embeddings)\n",
        "    return vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae83265a",
      "metadata": {
        "id": "ae83265a"
      },
      "outputs": [],
      "source": [
        "def ask_and_get_answer(vector_store, q, k=3):\n",
        "    from langchain.chains import RetrievalQA\n",
        "    from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.2)\n",
        "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
        "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
        "\n",
        "    answer = chain.run(q)\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c63da85",
      "metadata": {
        "id": "4c63da85"
      },
      "source": [
        "### RUNNING CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5de2f3aa",
      "metadata": {
        "id": "5de2f3aa"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db4952d9",
      "metadata": {
        "id": "db4952d9",
        "outputId": "47d318b4-4072-4724-9a48-8e29969c42c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading files/react.pdf\n",
            "You have 33 pages in your data\n"
          ]
        }
      ],
      "source": [
        "data = load_document(r\"D:\\Generative_AI\\Langchain\\react.pdf\")\n",
        "print (f'You have {len(data)} pages in your data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d960bb",
      "metadata": {
        "scrolled": true,
        "id": "16d960bb",
        "outputId": "e9e25e4a-c004-4cbf-a191-804f76d60959"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "528\n"
          ]
        }
      ],
      "source": [
        "chunks = chunk_data(data)\n",
        "print(len(chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf55a1d6",
      "metadata": {
        "id": "cf55a1d6"
      },
      "outputs": [],
      "source": [
        "vector_store = create_embeddings(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f90158d",
      "metadata": {
        "id": "4f90158d",
        "outputId": "08427289-9b88-43c4-c08d-06ecd62131de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reasoning and acting in LLMs refer to the integration of language models (LLMs) with the ability to reason and make decisions in a task-specific context. In this context, reasoning involves the process of logical thinking, inference, and problem-solving using the information provided in the language input. Acting, on the other hand, refers to the generation of task-specific actions based on the reasoning process. The goal is to combine these two capabilities in LLMs to create versatile and generalist agents that can perform a wide range of tasks requiring both reasoning and decision-making abilities.\n"
          ]
        }
      ],
      "source": [
        "# q = 'what is the whole document about?'\n",
        "q = 'what is resoning and acting in LLMs?'\n",
        "# q = 'Summarize the entire document in a few paragraphs.'\n",
        "\n",
        "k = 5\n",
        "answer = ask_and_get_answer(vector_store, q, k)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a2901f",
      "metadata": {
        "id": "44a2901f",
        "outputId": "14bb1596-c81c-4536-de1f-2785b5f9315c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write Quit of Exit to quit.\n",
            "Question #1: What is react?\n",
            "ReAct is a general paradigm that combines reasoning and acting with language models to solve various language reasoning and decision-making tasks. It is a method that utilizes large language models and allows for interaction with external sources of information. ReAct has been shown to outperform other methods in multiple trials, with performance gains ranging from 33% to 90% and averaging 62%.\n",
            "\n",
            " -------------------------------------------------- \n",
            "\n",
            "Question #2: quit\n",
            "Quitting ... Bye Bye!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "i = 1\n",
        "print('Write Quit of Exit to quit.')\n",
        "while True:\n",
        "    q = input(f'Question #{i}: ')\n",
        "    i = i+1\n",
        "    if q.lower() in  ['quit', 'exit']:\n",
        "        print('Quitting ... Bye Bye!')\n",
        "        time.sleep(2)\n",
        "        break\n",
        "\n",
        "    answer = ask_and_get_answer(vector_store, q, 5)\n",
        "    print(answer)\n",
        "    print(f'\\n {\"-\"*50} \\n')\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7dd6447",
      "metadata": {
        "id": "e7dd6447"
      },
      "outputs": [],
      "source": [
        "def ask_with_memory(vector_store, question, chat_history=[]):\n",
        "    from langchain.chains import ConversationalRetrievalChain\n",
        "    from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "    llm = ChatOpenAI(temperature=0.1)\n",
        "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5})\n",
        "    crc = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
        "    result = crc({\"question\": question, \"chat_history\": chat_history})\n",
        "    chat_history.append((question, result[\"answer\"]))\n",
        "\n",
        "    return result, chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6470b4bf",
      "metadata": {
        "id": "6470b4bf",
        "outputId": "df64bb44-0e74-4045-c668-6cad4c81eb47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The president said that Ketanji Brown Jackson is one of our nation's top legal minds and will continue Justice Breyer's legacy of excellence.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[('What did the president say about Ketanji Brown Jackson?', \"The president said that Ketanji Brown Jackson is one of our nation's top legal minds and will continue Justice Breyer's legacy of excellence.\")]\n"
          ]
        }
      ],
      "source": [
        "chat_history = list()\n",
        "\n",
        "q = 'What did the president say about Ketanji Brown Jackson?'\n",
        "result, chat_history = ask_with_memory(vector_store, q, chat_history)\n",
        "\n",
        "print(result['answer'])\n",
        "print('-' * 100)\n",
        "print(chat_history)  # for debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7263e27",
      "metadata": {
        "scrolled": true,
        "id": "c7263e27",
        "outputId": "3f7c3bb4-a6fb-483b-c3b4-43ea383428d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "He mentioned that Circuit Court of Appeals Judge Ketanji Brown Jackson will succeed Justice Breyer.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[('What did the president say about Ketanji Brown Jackson?', \"The president said that Ketanji Brown Jackson is one of our nation's top legal minds and will continue Justice Breyer's legacy of excellence.\"), ('Did he mention who she succeeded?', 'He mentioned that Circuit Court of Appeals Judge Ketanji Brown Jackson will succeed Justice Breyer.')]\n"
          ]
        }
      ],
      "source": [
        "q = 'Did he mention who she succeeded?'\n",
        "result, chat_history = ask_with_memory(vector_store, q, chat_history)\n",
        "\n",
        "print(result['answer'])\n",
        "print('-' * 100)\n",
        "print(chat_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac03e17",
      "metadata": {
        "id": "fac03e17"
      },
      "source": [
        "### Ask with Memory Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc8211d",
      "metadata": {
        "id": "dbc8211d"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "i = 1\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "print(\"Write Quit or Exit to quit\")\n",
        "while True:\n",
        "    q = input(f\"Question #{i}\")\n",
        "    i = i + 1\n",
        "    if q.lower() in [\"quit\",\"exit\"]:\n",
        "        print(\"Qutting\")\n",
        "        time.sleep(2)\n",
        "        break\n",
        "    result, _ = ask_with_memory(vector_store, q, chat_history)\n",
        "    print (result['answer'])\n",
        "    print(\"----------------------------------------------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}